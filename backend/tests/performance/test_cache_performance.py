#!/usr/bin/env python3

"""
æ€§èƒ½æµ‹è¯•ï¼šç¼“å­˜åŠŸèƒ½æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•
æµ‹è¯•ç¼“å­˜ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„è¡¨ç°
"""

import asyncio
import gc
import time

import aiohttp
import psutil
import pytest

from tests.factories import APITestDataFactory


class TestCachePerformance:
    """ç¼“å­˜åŠŸèƒ½æ€§èƒ½å’Œå‹åŠ›æµ‹è¯•"""

    @pytest.fixture
    async def api_client(self):
        """åˆ›å»ºé«˜æ€§èƒ½APIå®¢æˆ·ç«¯"""
        connector = aiohttp.TCPConnector(limit=0, force_close=True, enable_cleanup_closed=True)
        return aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=60, connect=5),
            connector=connector
        )

    @pytest.fixture
    def valid_headers(self):
        """æœ‰æ•ˆè®¤è¯å¤´éƒ¨"""
        return {"X-API-TOKEN": APITestDataFactory.create_valid_auth_token()}

    @pytest.fixture
    def base_url(self):
        """åŸºç¡€URL"""
        return "http://localhost:8000"

    @pytest.mark.performance
    async def test_api_response_time_benchmarks(self, api_client, valid_headers, base_url):
        """æµ‹è¯•APIå“åº”æ—¶é—´åŸºå‡†"""
        # Given
        test_url = f"{base_url}/health"
        create_data = {"novel_url": "https://example.com/novel/benchmark-test"}

        # When - æµ‹é‡å„ç«¯ç‚¹çš„å“åº”æ—¶é—´
        response_times = {}
        endpoints = [
            "health",
            "api/cache/create",
            "api/cache/status/1",
            "api/cache/tasks",
            "api/cache/cancel/1"
        ]

        for endpoint in endpoints:
            times = []
            for _ in range(10):  # æ¯ä¸ªç«¯ç‚¹æµ‹è¯•10æ¬¡
                start_time = time.time()

                if endpoint == "health":
                    async with api_client.get(test_url) as response:
                        assert response.status == 200
                        await response.text()
                elif endpoint == "api/cache/create":
                    async with api_client.post(f"{base_url}/{endpoint}", json=create_data, headers=valid_headers) as response:
                        # 201420çŠ¶æ€éƒ½è¢«è®¤ä¸ºæ˜¯æˆåŠŸçš„æ€§èƒ½æµ‹è¯•
                        assert response.status in [200, 400]
                else:
                    async with api_client.get(f"{base_url}/{endpoint}", headers=valid_headers) as response:
                        assert response.status in [200, 404, 401]

                times.append(time.time() - start_time)

            response_times[endpoint] = {
                "min": min(times),
                "max": max(times),
                "avg": sum(times) / len(times),
                "p95": sorted(times)[int(0.95 * len(times))]
            }

        # Then - æ€§èƒ½åŸºå‡†æ–­è¨€
        # å¥åº·çŠ¶æ€æ£€æŸ¥åº”è¯¥éå¸¸å¿«
        assert response_times["health"]["avg"] < 0.1  # 100ms
        assert response_times["health"]["p95"] < 0.2   # 200ms

        # ç¼“å­˜APIå“åº”æ—¶é—´åŸºå‡†
        assert response_times["api/cache/create"]["avg"] < 0.5  # 500ms
        assert response_times["api/cache/create"]["p95"] < 1.0   # 1s

        assert response_times["api/cache/status/1"]["avg"] < 0.2  # 200ms
        assert response_times["api/cache/status/1"]["p95"] < 0.5   # 500ms

        assert response_times["api/cache/tasks"]["avg"] < 0.3  # 300ms
        assert response_times["api/cache/tasks"]["p95"] < 0.6   # 600ms

        print("\nğŸ“Š APIå“åº”æ—¶é—´åŸºå‡† (ms):")
        for endpoint, times in response_times.items():
            print(f"  {endpoint}:")
            print(f"    å¹³å‡: {times['avg']*1000:.1f}")
            print(f"    P95: {times['p95']*1000:.1f}")
            print(f"    æœ€å°: {times['min']*1000:.1f}")
            print(f"    æœ€å¤§: {times['max']*1000:.1f}")

    @pytest.mark.performance
    async def test_concurrent_cache_requests_performance(self, api_client, valid_headers, base_url):
        """æµ‹è¯•å¹¶å‘ç¼“å­˜è¯·æ±‚æ€§èƒ½"""
        # Given
        concurrent_levels = [10, 50, 100, 200]
        results = {}

        for level in concurrent_levels:
            # When
            start_time = time.time()
            success_count = 0
            error_count = 0

            async def create_cache_task(task_index):
                try:
                    create_data = {"novel_url": f"https://example.com/novel/concurrent-{level}-{task_index}"}
                    async with api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers) as response:
                        if response.status in [200, 400, 422]:  # æˆåŠŸæˆ–é¢„æœŸé”™è¯¯
                            return response.status
                        else:
                            return None
                except Exception:
                    return None

            # å¹¶å‘æ‰§è¡Œè¯·æ±‚
            tasks = [create_cache_task(i) for i in range(level)]
            responses = await asyncio.gather(*tasks, return_exceptions=True)

            end_time = time.time()

            # è®¡ç®—ç»“æœ
            for response in responses:
                if isinstance(response, int) and response in [200, 400, 422]:
                    success_count += 1
                else:
                    error_count += 1

            duration = end_time - start_time
            results[level] = {
                "duration": duration,
                "success_rate": success_count / level,
                "error_rate": error_count / level,
                "requests_per_second": level / duration
            }

        # Then - æ€§èƒ½åˆ†æ
        for level, result in results.items():
            # æˆåŠŸç‡åº”è¯¥ä¿æŒè¾ƒé«˜æ°´å¹³
            assert result["success_rate"] > 0.95, f"å¹¶å‘çº§åˆ« {level} çš„æˆåŠŸç‡è¿‡ä½: {result['success_rate']}"

            # å“åº”æ—¶é—´åº”è¯¥éšå¹¶å‘æ•°åˆç†å¢é•¿
            if level > 10:
                prev_result = results[level // 2 * 2]  # æ‰¾åˆ°å‰ä¸€ä¸ªå¹¶å‘çº§åˆ«
                # å“åº”æ—¶é—´å¢é•¿åº”è¯¥å°äºçº¿æ€§
                time_ratio = result["duration"] / prev_result["duration"]
                assert time_ratio < 1.5, f"å¹¶å‘ {level} vs {level//2*2} æ—¶é—´å¢é•¿ä¸åˆç†: {time_ratio:.2f}x"

            # æ¯ç§’è¯·æ±‚æ•°åº”è¯¥æœ‰åˆç†ä¸Šé™
            if level <= 50:
                assert result["requests_per_second"] > 50, f"å¹¶å‘ {level} çš„ååé‡è¿‡ä½"
            elif level <= 100:
                assert result["requests_per_second"] > 30, f"å¹¶å‘ {level} çš„ååé‡è¿‡ä½"

        print("\nğŸ“ˆ å¹¶å‘æ€§èƒ½æµ‹è¯•ç»“æœ:")
        for level, result in results.items():
            print(f"  å¹¶å‘çº§åˆ« {level}:")
            print(f"    è€—æ—¶: {result['duration']:.2f}s")
            print(f"    æˆåŠŸç‡: {result['success_rate']:.1%}")
            print(f"    RPS: {result['requests_per_second']:.1f}")

    @pytest.mark.performance
    async def test_large_data_handling_performance(self, api_client, valid_headers, base_url):
        """æµ‹è¯•å¤§æ•°æ®å¤„ç†æ€§èƒ½"""
        # Given - æ¨¡æ‹Ÿå¤§é‡ç¼“å­˜ä»»åŠ¡å’Œç« èŠ‚
        large_task_count = 100
        chapters_per_task = 1000

        # When - åˆ›å»ºå¤§é‡ä»»åŠ¡ï¼ˆè¿™ä¸ªå¯èƒ½å¾ˆæ…¢ï¼Œæ‰€ä»¥æ ‡è®°ä¸ºperformanceï¼‰
        start_time = time.time()
        task_ids = []

        for i in range(large_task_count):
            create_data = {"novel_url": f"https://example.com/novel/large-data-{i}"}
            async with api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers) as response:
                if response.status == 200:
                    result = await response.json()
                    task_ids.append(result["task_id"])
                # 400é”™è¯¯ä¹Ÿæ˜¯é¢„æœŸçš„ï¼ˆå› ä¸ºURLå¯èƒ½æ— æ•ˆï¼‰
                elif response.status == 400:
                    pass
                else:
                    pytest.fail(f"åˆ›å»ºå¤§æ•°æ®ä»»åŠ¡ {i} å¤±è´¥: {response.status}")

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©åç«¯å¤„ç†
        await asyncio.sleep(2)

        creation_time = time.time() - start_time

        # Then - æ€§èƒ½åˆ†æ
        assert creation_time < 60, f"åˆ›å»º {large_task_count} ä¸ªä»»åŠ¡è€—æ—¶è¿‡é•¿: {creation_time:.2f}s"
        assert len(task_ids) > large_task_count * 0.8, f"ä»»åŠ¡åˆ›å»ºæˆåŠŸç‡è¿‡ä½: {len(task_ids)}/{large_task_count}"

        # æµ‹è¯•è·å–å¤§é‡ä»»åŠ¡åˆ—è¡¨çš„æ€§èƒ½
        start_time = time.time()

        async with api_client.get(f"{base_url}/api/cache/tasks?limit={large_task_count}", headers=valid_headers) as response:
            assert response.status == 200
            result = await response.json()
            assert len(result["tasks"]) <= large_task_count

        list_time = time.time() - start_time
        assert list_time < 5, f"è·å– {large_task_count} ä¸ªä»»åŠ¡åˆ—è¡¨è€—æ—¶è¿‡é•¿: {list_time:.2f}s"

        print("\nğŸ“Š å¤§æ•°æ®æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  åˆ›å»ºä»»åŠ¡æ•°: {large_task_count}")
        print(f"  åˆ›å»ºè€—æ—¶: {creation_time:.2f}s")
        print(f"  å¹³å‡åˆ›å»ºæ—¶é—´: {creation_time/large_task_count:.3f}s/ä»»åŠ¡")
        print(f"  ä»»åŠ¡åˆ—è¡¨è€—æ—¶: {list_time:.2f}s")
        print(f"  æˆåŠŸåˆ›å»ºä»»åŠ¡: {len(task_ids)}")

    @pytest.mark.performance
    async def test_memory_usage_monitoring(self, api_client, valid_headers, base_url):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨ç›‘æ§"""
        # Given
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        print(f"\nğŸ§  åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.1f} MB")

        # When - æ‰§è¡Œå†…å­˜å¯†é›†æ“ä½œ
        memory_snapshots = []

        # ç¬¬ä¸€é˜¶æ®µï¼šåˆ›å»ºå¤§é‡ä»»åŠ¡
        for batch in range(5):  # 5ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹20ä¸ªä»»åŠ¡
            tasks = []
            for i in range(20):
                create_data = {"novel_url": f"https://example.com/novel/memory-test-{batch}-{i}"}
                tasks.append(api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers))

            await asyncio.gather(*tasks)

            # è®°å½•å†…å­˜å¿«ç…§
            gc.collect()
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_snapshots.append(current_memory)

            # å¼ºåˆ¶ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            await asyncio.sleep(0.5)

        # ç¬¬äºŒé˜¶æ®µï¼šè·å–ä»»åŠ¡åˆ—è¡¨
        for _ in range(10):  # 10æ¬¡æŸ¥è¯¢
            async with api_client.get(f"{base_url}/api/cache/tasks", headers=valid_headers) as response:
                await response.text()

        # è®°å½•æœ€ç»ˆå†…å­˜
        gc.collect()
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_snapshots.append(final_memory)

        # Then - å†…å­˜ä½¿ç”¨åˆ†æ
        max_memory = max(memory_snapshots)
        memory_growth = final_memory - initial_memory

        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert memory_growth < 100, f"å†…å­˜å¢é•¿è¿‡å¤š: {memory_growth:.1f} MB"

        # æœ€å¤§å†…å­˜ä½¿ç”¨åº”è¯¥æœ‰ä¸Šé™
        assert max_memory < 500, f"æœ€å¤§å†…å­˜ä½¿ç”¨è¿‡å¤š: {max_memory:.1f} MB"

        print("\nğŸ§  å†…å­˜ä½¿ç”¨ç›‘æ§ç»“æœ:")
        print(f"  åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory:.1f} MB")
        print(f"  å†…å­˜å¢é•¿: {memory_growth:.1f} MB")
        print(f"  å³°å€¼å†…å­˜: {max_memory:.1f} MB")
        print(f"  å†…å­˜å¿«ç…§: {[f'{m:.1f}' for m in memory_snapshots]} MB")

    @pytest.mark.performance
    async def test_database_performance(self, api_client, valid_headers, base_url):
        """æµ‹è¯•æ•°æ®åº“æ€§èƒ½"""
        # Given
        db_operation_times = []

        # When - æµ‹è¯•æ•°æ®åº“å¯†é›†æ“ä½œ
        for i in range(50):  # 50æ¬¡æ•°æ®åº“æŸ¥è¯¢
            start_time = time.time()

            async with api_client.get(f"{base_url}/api/cache/tasks", headers=valid_headers) as response:
                if response.status == 200:
                    await response.json()

            end_time = time.time()
            db_operation_times.append(end_time - start_time)

        # Then - æ€§èƒ½åˆ†æ
        avg_time = sum(db_operation_times) / len(db_operation_times)
        p95_time = sorted(db_operation_times)[int(0.95 * len(db_operation_times))]

        assert avg_time < 0.2, f"å¹³å‡æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿: {avg_time:.3f}s"
        assert p95_time < 0.5, f"P95æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿: {p95_time:.3f}s"

        print("\nğŸ’¾ æ•°æ®åº“æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  æŸ¥è¯¢æ¬¡æ•°: {len(db_operation_times)}")
        print(f"  å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        print(f"  P95æ—¶é—´: {p95_time:.3f}s")
        print(f"  æœ€å¿«: {min(db_operation_times):.3f}s")
        print(f"  æœ€æ…¢: {max(db_operation_times):.3f}s")

    @pytest.mark.performance
    async def test_websocket_performance(self, api_client, valid_headers, base_url):
        """æµ‹è¯•WebSocketæ€§èƒ½"""
        # Given
        create_data = {"novel_url": "https://example.com/novel/websocket-test"}

        async with api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers) as response:
            if response.status != 200:
                pytest.skip("æ— æ³•åˆ›å»ºWebSocketæµ‹è¯•ä»»åŠ¡")
            result = await response.json()
            task_id = result["task_id"]

        # When - æµ‹è¯•WebSocketè¿æ¥å’Œæ¶ˆæ¯æ€§èƒ½
        ws_url = f"ws://localhost:8000/ws/cache/{task_id}"
        connection_time = None
        message_times = []

        try:
            start_time = time.time()
            async with api_client.ws_connect(ws_url) as ws:
                connection_time = time.time() - start_time

                # æµ‹è¯•è¿æ¥æ—¶é—´
                assert connection_time < 1.0, f"WebSocketè¿æ¥æ—¶é—´è¿‡é•¿: {connection_time:.3f}s"

                # æµ‹è¯•æ¶ˆæ¯æ¥æ”¶æ€§èƒ½
                message_count = 0
                timeout = aiohttp.ClientTimeout(total=5)

                while message_count < 10:  # æ¥æ”¶10ä¸ªæ¶ˆæ¯
                    try:
                        msg = await asyncio.wait_for(ws.receive_msg(), timeout=timeout)
                        msg_start = time.time()

                        if msg.type == aiohttp.WSMsgType.TEXT:
                            data = json.loads(msg.data)
                            message_times.append(time.time() - msg_start)
                            message_count += 1
                        elif msg.type == aiohttp.WSMsgType.ERROR or msg.type == aiohttp.WSMsgType.CLOSED:
                            break
                    except TimeoutError:
                        break

                if message_count >= 10 or ws.closed:
                    break

        except Exception as e:
            pytest.skip(f"WebSocketæµ‹è¯•å¤±è´¥: {e}")

        # Then - WebSocketæ€§èƒ½åˆ†æ
        if connection_time:
            assert connection_time < 1.0, "WebSocketè¿æ¥æ€§èƒ½ä¸è¾¾æ ‡"

        if message_times:
            avg_message_time = sum(message_times) / len(message_times)
            assert avg_message_time < 0.1, f"å¹³å‡æ¶ˆæ¯æ—¶é—´è¿‡é•¿: {avg_message_time:.3f}s"

        print("\nğŸŒ WebSocketæ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  è¿æ¥æ—¶é—´: {connection_time:.3f}s" if connection_time else "N/A")
        print(f"  æ¶ˆæ¯æ•°é‡: {len(message_times)}")
        if message_times:
            print(f"  å¹³å‡æ¶ˆæ¯æ—¶é—´: {avg_message_time:.3f}s")
            print(f"  æ¶ˆæ¯æ—¶é—´èŒƒå›´: {min(message_times):.3f}s - {max(message_times):.3f}s")

    @pytest.mark.stress
    async def test_sustained_load_stress_test(self, api_client, valid_headers, base_url):
        """æµ‹è¯•æŒç»­è´Ÿè½½å‹åŠ›æµ‹è¯•"""
        # Given
        duration = 30  # 30ç§’å‹åŠ›æµ‹è¯•
        rps_target = 10  # ç›®æ ‡æ¯ç§’10ä¸ªè¯·æ±‚
        total_requests = int(duration * rps_target)

        # When
        start_time = time.time()
        success_count = 0
        error_count = 0
        response_times = []

        async def sustained_requests():
            while time.time() - start_time < duration:
                request_start = time.time()
                create_data = {"novel_url": f"https://example.com/novel/stress-test-{int(time.time())}"}

                try:
                    async with api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers) as response:
                        response_time = time.time() - request_start
                        response_times.append(response_time)

                        if response.status in [200, 400, 422]:
                            success_count += 1
                        else:
                            error_count += 1

                except Exception:
                    error_count += 1

                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                elapsed = time.time() - request_start
                sleep_time = max(0, (1.0 / rps_target) - elapsed)
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)

        # å¯åŠ¨å¤šä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹æ¨¡æ‹ŸçœŸå®è´Ÿè½½
        concurrent_workers = 3
        tasks = [sustained_requests() for _ in range(concurrent_workers)]
        await asyncio.gather(*tasks)

        end_time = time.time()
        actual_duration = end_time - start_time

        # Then - å‹åŠ›æµ‹è¯•åˆ†æ
        total_requests_processed = success_count + error_count
        actual_rps = total_requests_processed / actual_duration

        # éªŒè¯æŒç»­æ€§èƒ½
        assert actual_rps >= rps_target * 0.8, f"å®é™…RPS {actual_rps:.1f} ä½äºç›®æ ‡ {rps_target} çš„80%"
        assert success_count / total_requests_processed > 0.95, f"æˆåŠŸç‡è¿‡ä½: {success_count/total_requests_processed:.1%}"

        # éªŒè¯å“åº”æ—¶é—´ç¨³å®šæ€§
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            p95_response_time = sorted(response_times)[int(0.95 * len(response_times))]
            assert avg_response_time < 2.0, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.3f}s"
            assert p95_response_time < 5.0, f"P95å“åº”æ—¶é—´è¿‡é•¿: {p95_response_time:.3f}s"

        print(f"\nğŸ”¥ å‹åŠ›æµ‹è¯•ç»“æœ (æŒç»­æ—¶é—´: {actual_duration:.1f}s):")
        print(f"  ç›®æ ‡RPS: {rps_target}")
        print(f"  å®é™…RPS: {actual_rps:.1f}")
        print(f"  æ€»è¯·æ±‚æ•°: {total_requests_processed}")
        print(f"  æˆåŠŸè¯·æ±‚: {success_count}")
        print(f"  å¤±è´¥è¯·æ±‚: {error_count}")
        print(f"  æˆåŠŸç‡: {success_count/total_requests_processed:.1%}")
        if response_times:
            print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
            print(f"  P95å“åº”æ—¶é—´: {p95_response_time:.3f}s")

    @pytest.mark.performance
    def test_system_resource_limits(self):
        """æµ‹è¯•ç³»ç»Ÿèµ„æºé™åˆ¶"""
        # Given
        cpu_count = psutil.cpu_count()
        memory_info = psutil.virtual_memory()

        print("\nğŸ’» ç³»ç»Ÿèµ„æºä¿¡æ¯:")
        print(f"  CPUæ ¸å¿ƒæ•°: {cpu_count}")
        print(f"  æ€»å†…å­˜: {memory_info.total / 1024 / 1024:.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {memory_info.available / 1024 / 1024:.1f} GB")
        print(f"  å†…å­˜ä½¿ç”¨ç‡: {(1 - memory_info.available/memory_info.total)*100:.1f}%")

        # éªŒè¯ç³»ç»Ÿèµ„æºæ»¡è¶³æµ‹è¯•è¦æ±‚
        assert cpu_count >= 2, "ç³»ç»ŸCPUæ ¸å¿ƒæ•°ä¸è¶³"
        assert memory_info.total / 1024 / 1024 >= 4, "ç³»ç»Ÿå†…å­˜ä¸è¶³ (éœ€è¦è‡³å°‘4GB)"

        # When - æ£€æŸ¥å½“å‰è¿›ç¨‹èµ„æºä½¿ç”¨
        process = psutil.Process()
        process_cpu = process.cpu_percent(interval=1)
        process_memory = process.memory_info()
        process_threads = process.num_threads()

        # Then - éªŒè¯è¿›ç¨‹èµ„æºä½¿ç”¨åœ¨åˆç†èŒƒå›´
        assert process_memory.rss / 1024 / 1024 < 512, "è¿›ç¨‹å†…å­˜ä½¿ç”¨è¿‡é«˜: {process_memory.rss/1024/1024:.1f} MB"
        assert process_threads <= 20, "è¿›ç¨‹çº¿ç¨‹æ•°è¿‡å¤š: {process_threads}"

        print(f"  è¿›ç¨‹å†…å­˜ä½¿ç”¨: {process_memory.rss / 1024 / 1024:.1f} MB")
        print(f"  è¿›ç¨‹CPUä½¿ç”¨: {process_cpu:.1f}%")
        print(f"  è¿›ç¨‹çº¿ç¨‹æ•°: {process_threads}")

    @pytest.mark.performance
    async def test_cache_service_resource_cleanup(self, api_client, valid_headers, base_url):
        """æµ‹è¯•ç¼“å­˜æœåŠ¡èµ„æºæ¸…ç†"""
        # Given - ç›‘æ§åˆå§‹èµ„æº
        initial_memory = psutil.Process().memory_info().rss
        initial_threads = psutil.Process().num_threads()

        # When - æ‰§è¡Œå¤§é‡æ“ä½œåæ¸…ç†
        task_ids = []

        # åˆ›å»º100ä¸ªä»»åŠ¡
        for i in range(100):
            create_data = {"novel_url": f"https://example.com/novel/cleanup-{i}"}
            async with api_client.post(f"{base_url}/api/cache/create", json=create_data, headers=valid_headers) as response:
                if response.status == 200:
                    result = await response.json()
                    task_ids.append(result["task_id"])

        # è·å–ä»»åŠ¡åˆ—è¡¨å¤šæ¬¡
        for _ in range(20):
            async with api_client.get(f"{base_url}/api/cache/tasks", headers=valid_headers) as response:
                if response.status == 200:
                    await response.text()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©èµ„æºç¨³å®š
        await asyncio.sleep(2)

        # ç›‘æ§æ¸…ç†åçš„èµ„æº
        final_memory = psutil.Process().memory_info().rss
        final_threads = psutil.Process().num_threads()

        # Then - éªŒè¯èµ„æºæ¸…ç†æ•ˆæœ
        memory_diff = final_memory - initial_memory
        threads_diff = final_threads - initial_threads

        print("\nğŸ§¹ èµ„æºæ¸…ç†æµ‹è¯•:")
        print(f"  åˆå§‹å†…å­˜: {initial_memory / 1024 / 1024:.1f} MB")
        print(f"  æœ€ç»ˆå†…å­˜: {final_memory / 1024 / 1024:.1f} MB")
        print(f"  å†…å­˜å·®å¼‚: {memory_diff / 1024 / 1024:.1f} MB")
        print(f"  åˆå§‹çº¿ç¨‹: {initial_threads}")
        print(f"  æœ€ç»ˆçº¿ç¨‹: {final_threads}")
        print(f"  çº¿ç¨‹å·®å¼‚: {threads_diff}")

        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆè€ƒè™‘æµ‹è¯•è¿‡ç¨‹ä¸­äº§ç”Ÿçš„æ•°æ®ï¼‰
        assert memory_diff < 200, f"å†…å­˜å¢é•¿è¿‡å¤š: {memory_diff / 1024 / 1024:.1f} MB"
